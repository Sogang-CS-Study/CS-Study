## CS 스터디 5주차

### CPU 스케줄링
* 스케줄링은 여러 프로세스가 번갈아 사용하는 자원(=프로세서)를 어떤 시점에 어떤 프로세스에 할당할지 결정하는 것이다. 스케줄링이 잘 이루어진다면, 프로세서의 효율성이 높아지고, 작업(프로세스)의 응답시간이 최소화되어 시스템의 작업 처리 능력이 향상된다.

| 스케줄링이 필요하지 않은 프로세스                               | 스케줄링이 필요한 프로세스                                |
|-----------------------------------------------------------------|-----------------------------------------------------------|
| 인터럽트 처리,  오류 처리,  사용자의 시스템 호출 등의 사전 처리 | 사용자 프로세스,   시스템 호출로 발생하는 시스템 프로세스 |

**- 프로세스의 동작**
* 프로세스 버스트(process burst): 프로세스를 프로세서에서 실행할 때
* 입출력 버스트: 프로세스가 추가로 실행하려고 입출력을 기다리고 있을 때
* 프로세스 버스트가 짧고 입출력 버스트가 긴 프로세스를 입출력 중심 프로세스라 한다. 이와 반대로 프로세스 버스트가 길고 입출력 버스트가 짧은 프로세스를 프로세서 중심 프로세스라고 한다.

**- 스케줄링의 단계**
* (1단계) 작업 스케줄링
- 디스크에 있는 작업 중 실제로 시스템 자원을 사용할 작업을 결정하고, 작업을 프로세스들로 나눠 생성한다.  
* (2단계) 작업 승인과 프로세서 결정 스케줄링
- 프로세서를 사용할 권한을 가질 프로세스를 결정하는 '작업 승인'과, '프로세서 결정 스케줄링'을 한다.
- 시스템의 오버헤드에 따라 연기할 프로세스를 잠정적으로 결정하면서, 1단계 스케줄링과 3단계 스케줄링의 완충 역할을 한다.
* (3단계) 프로세서 할당 스케줄링 : 준비 상태의 프로세스에 프로세서 할당(디스패칭)
- 이 단계에서는 프로세서를 할당할 프로세스를 결정한다. 단, 프로세스는 디스패처(분배기)가 준비 상태에 있어야 한다.

**- 스케줄링 큐**
* 스케줄링 큐는 프로세스 제어 블록의 연결 리스트 형태로 구현되어 있다.
* 스케줄링 큐는 크게 '준비 큐'와 '입출력장치 큐'로 나눌 수 있다. 
* 준비 큐는 시스템에 하나씩만 있으며, 프로세서를 할당받아 실행되기를 기다리는 프로세스들이 대기한다. 좁은 의미에서 스케줄링은 준비 큐에서 프로세스를 하나 선택하는 것이다.
* 입출력장치 큐는 하나의 시스템에 여러 개가 있을 수 있다(ex.  프린터, 디스크, 단말기 등). 전용장치의 경우(ex. 테이프 드라이브, 시분할 단말기 등)에 입출력장치 큐는 하나의 프로세스만 가질 수 있는 반면, 디스크와 같이 공유할 수 있는 장치의 경우에 입출력장치 큐에 여러 프로세스가 대기할 수 있다.


### 캐시(Cache)란
* 캐시 메모리는 컴퓨터 시스템에서 중요한 역할을 하는데, 속도 차이에 따른 병목 현상을 줄이고 성능을 향상시키는 역할을 한다. 

**- 캐시 메모리의 역할과 기능**
* 캐시 메모리는 컴퓨터의 중요한 부분 중 하나로, CPU와 주기억장치 간의 데이터 전송 속도 차이를 극복하기 위한 중간 저장 공간으로 사용된다. CPU는 주기억장치에서 데이터를 읽어오는데, 이때 자주 사용하는 데이터를 캐시 메모리에 저장한 후, 다음에 필요할 때 캐시 메모리에서 먼저 가져오면서 속도를 향상시킨다. 이러한 방식은 빠른 속도를 제공하지만, 캐시 메모리의 용량은 제한적이며 비용이 비싸다는 단점이 있다. CPU에는 일반적으로 L1, L2, L3 캐시 메모리가 있으며, 이들은 속도와 크기에 따라 분류된다. 일반적으로 L1 캐시가 가장 먼저 사용되며, 데이터를 찾지 못할 경우 L2로 이동한다.

* 코어 프로세서의 경우 각 코어마다 독립된 L1 캐시 메모리를 가지고 있으며, 두 코어가 공유하는 L2 캐시 메모리가 내장되어 있다. 이것은 캐시 메모리의 구조 중 하나로, 캐시의 효율적인 사용을 위한 중요한 구성 요소이다.


**- 캐시 메모리의 작동 원리**
* 시공간 지역성: 시간 지역성은 한 번 참조된 데이터가 잠시 후에 다시 참조될 가능성이 높다는 원리를 의미하며, 공간 지역성은 연속적으로 참조되는 데이터 근처에 있는 데이터가 잠시 후에 다시 사용될 가능성이 높다는 원리를 나타낸다. 따라서 캐시에 데이터를 저장할 때는 이러한 지역성을 활용하여 해당 데이터뿐만 아니라 인접한 데이터도 함께 가져와 미래 사용에 대비한다.

* 캐시 메모리의 구조 및 작동 방식은 Direct Mapped Cache, Fully Associative Cache, Set Associative Cache 등으로 나뉜다. 

* 캐시 메모리의 쓰기 정책은 Write Through와 Write Back으로 나뉜다. 

* 또한, 주소 매핑 방식은 Direct Mapping, Associative Mapping, Set Associative Mapping 등이 있으며, 이를 통해 캐시와 주기억장치의 주소 관리를 한다.

* 마지막으로, 캐시 메모리의 교체 알고리즘으로는 FIFO, LRU, LFU, OPTIMAL 방식이 있으며, 각각의 방식은 특정 상황에 더 효율적이다.

**- 캐시 미스**
* Cold miss: 처음 불러오는 경우
* Conflict miss: 데이터 충돌로 인한 미스
* Capacity miss: 용량 부족으로 인한 미스

### 데드락(DeadLock)
* 데드락은 서로 상대방의 작업이 끝나기만을 기다리고 있기 때문에 결과적으로 아무것도 못하는 상태이다. 데드락은 주로 네 가지 조건으로 발생한다: 상호 배제, 점유 대기, 비선점, 순환 대기.

* 상호 배제 조건은 자원을 한 번에 하나의 프로세스만 사용할 수 있도록 하는 것을 의미한다. 이것을 제거하는 것은 일반적으로 불가능하다. 

* 점유 대기 조건은 프로세스가 이미 자원을 보유한 상태에서 다른 자원을 기다릴 때 발생한다. 이 조건을 없애기 위해 모든 필요한 자원을 한 번에 할당하거나, 자원을 요청할 때 보유한 자원을 모두 놓고 다시 요청하는 방법이 있다. 그러나 이렇게 하면 자원 이용률이 감소하고 오버헤드가 발생할 수 있다.

* 비선점 조건은 자원을 점유한 프로세스가 자발적으로 자원을 내놓기 전까지는 다른 프로세스에게 선점되지 않는다는 것이다. 이 조건을 없애기 위해선 자원을 선점할 수 있는 방법을 도입해야 한다.

* 순환 대기 조건은 프로세스가 자원을 순환적으로 기다릴 때 발생한다. 이를 방지하기 위해 자원 요청에 대한 순서를 정해야 한다.

**- 데드락을 예방하는 방법**
* 데드락을 예방하는 방법으로는 상호 배제 조건을 제거하거나, 점유 대기 조건을 제거하거나, 비선점 조건을 제거하거나, 순환 대기 조건을 제거하는 방법이 있다. 그러나 이러한 방법들은 효율성이나 구현의 어려움 등 다양한 제약 사항이 따르며, 일반적으로 모든 조건을 동시에 제거하는 것은 어렵다.

* 데드락을 회피하는 방법으로는 자원 할당 그래프 알고리즘과 은행원 알고리즘이 있다. 자원 할당 그래프 알고리즘은 자원 할당 상태를 검사하여 데드락 가능성을 판단하고, 안전한 경우에만 자원을 할당한다. 은행원 알고리즘은 프로세스가 자원을 요청할 때 시스템이 안전 상태를 유지할 수 있는지를 미리 판단하여 자원을 할당한다.

**- 데드락에 대한 대처 방법**
* 데드락이 발생하면 복구하기 위해 프로세스 종료 또는 자원 뺏기와 같은 방법을 사용할 수 있다. 그러나 이러한 방법은 프로세스의 중단이나 자원의 선점으로 인한 오버헤드가 발생할 수 있다.

* 마지막으로, 데드락을 무시하는 방법도 있지만, 이것은 데드락이 발생할 때마다 시스템을 멈추고 복구하는 것이 아니라 무시하고 계속 작업을 수행하는 방법이다. 이 방법은 데드락이 드물게 발생하는 경우에 사용할 수 있으며, 대부분의 범용 운영 체제가 이 방법을 채택하고 있다.

### Race Condition
**- Race Condition의 정의**
* Race Condition은 여러 프로세스나 스레드가 동시에 공유 자원에 접근할 때 발생하는 비결정적인 결과를 일으키는 상태를 말한다. 동시 접근이 발생하는 공유 자원을 임계 영역(Critical Section)이라고 부른다.
* 공유 자원에 동시 접근으로 인한 예상치 못한 결과가 생길 수 있으므로 이를 방지할 수 있어 한다. 

**- 임계 영역 문제의 해결 방법**
* 임계 영역 문제(Critical Section Problem)는 임계 영역에 접근하려는 다수의 프로세스나 스레드가 Race Condition을 일으키지 않도록 하는 문제이며, 이를 해결하기 위한 방법은 다음과 같다.

1. 상호배제(Mutual Exclusion)
* 한 프로세스가 임계 영역에서 실행 중일 때, 다른 프로세스는 해당 영역에 접근할 수 없으며 작업이 끝날 때까지 기다려야 한다. 
* 이는 Mutex나 Lock을 사용하여 구현할 수 있다.
* Lock은 하드웨어 기반 해결책으로, Critical Section에 진입하는 프로세스가 Lock을 획득하고 나갈 때 Lock을 방출함으로써 동시 접근을 막는다. Mutex는 Lock이 걸려있으면 대기하며 컨텍스트 스위칭을 진행할 수 있지만, 자원이 단시간 내로 얻을 수 있다면 컨텍스트 스위칭이 더 큰 낭비가 될 수 있다.

2. 진행(Progress)
* 임계 영역에서 실행 중인 프로세스가 없으면, 다른 프로세스들 중에서 별도의 동작이 없는 프로세스만 임계 영역에 접근할 수 있는 후보가 된다. 

3. 한정 대기(Bounded Waiting)
* Critical Section에 진입을 신청한 후 다른 프로세스가 진입하는 횟수를 제한함으로써 기아 상태를 방지한다.

### 세마포어(Semaphore) & 뮤텍스(Mutex)
* Mutex와 Semaphore는 공유 자원에 대한 동기화를 달성하기 위한 소프트웨어 도구로 사용된다. Race Condition과 같은 문제를 해결하고 여러 프로세스나 스레드 간의 동시 접근을 효과적으로 관리하는 데 도움이 된다.

**- 뮤텍스**
* 뮤텍스(Mutex) Mutual Exclusion의 축약어로, 상호 배제를 의미한다.
* 뮤텍스는 자원 접근을 차단하여 한 번에 하나의 스레드나 프로세스만 실행되도록 보장한다. 
* acquire()와 release() 함수 호출을 통해 동작한다. 이를 통해 한 스레드나 프로세스가 자원에 접근하면 다른 스레드나 프로세스의 접근을 차단하여 Race Condition을 해결한다.

**- 세마포어**
* Semaphore는 Mutex와 유사하지만 보다 정교한 동기화를 가능하게 하는 방법을 제공한다. Counting Semaphore와 Binary Semaphore 두 가지 주요 종류가 있다.
* Counting Semaphore는 0 이상의 정수 값을 갖는 도메인을 가지며 주로 자원 카운팅에 사용된다. 
* 반면 Binary Semaphore는 0 또는 1 값을 가질 수 있으며 주로 상호 배제(락 및 언락)에 사용된다.
* 세마포어는 숫자를 통해 자원 상태를 나타내며 lock을 설정하고 signal을 보내는 등 보다 복잡한 동기화를 가능하게 한다.

* 세마포어와 뮤텍스는 다중 프로세스나 스레드 환경에서 Critical Section 문제를 해결하기 위한 중요한 수단이지만, Busy Waiting과 같은 단점도 가지고 있다. 이러한 단점을 해결하기 위해 대기 상태에 있는 프로세스를 블록시키고, 자원이 해제될 때 깨우는 방식을 사용한다.

### 페이징 & 세그먼테이션 (PDF)
* 페이징과 세그멘테이션, 이 두 가지 메모리 관리 방법은 프로세스의 가상 주소 공간을 물리 메모리에 매핑하는 역할을 하며, 이는 메모리 관리에서 중요한 역할이다. 성능과 효율성을 향상시키기 위한 다양한 최적화 기법과 함께 사용된다. 페이징과 세그멘테이션은 각각의 장단점이 있으며, 어떤 방법을 선택할지는 메모리 관리 요구사항에 따라 결정된다. 

**- 세그멘테이션**
* 세그멘테이션은 MMU(Memory Management Unit)에 하나의 base와 bound가 존재하는 대신, 세그먼트마다 base와 bound를 사용한다.
* 세그먼트는 코드, 스택, 힙의 세 가지 종류의 연속적인 주소 공간을 의미하며, 이를 통해 운영 체제는 각 세그먼트를 물리 메모리의 다른 위치에 배치할 수 있고 사용하지 않는 주소 공간이 메모리를 차지하는 것을 막을 수 있다. 이렇게 함으로써 메모리의 단편화를 줄일 수 있다.

* 세그멘테이션에서는 각 세그먼트가 시작하고 크기가 어떻게 되는지를 레지스터에 기록하여, 가상 주소에서 실제 주소로 변환된다. 주소 변환은 base와 offset을 조합하여 이루어지며, 스택은 거꾸로 확장하므로 base - offset을 사용하여 주소를 계산한다.

**- 페이징**
* 반면, 페이징은 세그멘테이션과 달리 고정된 크기의 페이지로 가상 주소 공간을 분할한다. 각 페이지는 페이지 프레임으로 물리 메모리와 일치시킨다. 페이징을 사용하면 프로세스의 주소 공간을 효율적으로 지원할 수 있으며 스택과 힙의 확장 방향과 사용 방식을 고려하지 않아도 된다. 또한 단순한 배치가 가능하며 페이지 테이블을 사용하여 가상 주소를 물리 주소로 변환한다.

* 페이지 테이블은 가상 페이지와 물리 페이지 간의 매핑 정보를 저장하는 자료 구조로, 각 가상 페이지에 대한 물리 메모리 위치를 기록한다. 페이지 테이블을 통해 가상 주소에서 물리 주소로의 변환이 이루어진다. 페이지 테이블의 일부 엔트리는 TLB (Translation Lookaside Buffer)라는 캐시 메모리에 저장된다. TLB는 자주 참조되는 페이지 번호와 프레임 번호를 저장하여 주소 변환을 빠르게 수행한다. 페이지 테이블은 각 프로세스마다 별도로 유지되며 주소 공간의 각 가상 페이지에 대한 물리 메모리 위치를 기록한다.


### 페이지 교체 알고리즘
**- 페이지 교체의 정의**
* 메모리(RAM)은 하드 디스크나 주기억 장치의 일부를 가져와서 메모리에 넣어 놓는다. 그러나 메모리에는 주기억 장치의 전체를 저장할 수 없으므로 페이지 교체가 필요하다. 페이지 교체는 어떤 페이지를 메모리에 올릴지, 어떤 페이지를 디스크에 내릴지 결정하는 과정을 의미한다.

**- 페이지 교체 알고리즘**
* 가장 이상적인 페이지 교체 알고리즘은 Optimal 페이지 교체로, 미래를 예측하여 가장 오랫동안 사용되지 않을 페이지를 교체한다. 그러나 이러한 미래 예측은 현실적으로 불가능하다.

* FIFO 페이지 교체 알고리즘: 먼저 들어온 페이지를 먼저 내보내는 가장 간단한 방법이다. 그러나 Belady's Anomaly라는 현상으로 인해 페이지 폴트 횟수가 증가할 수 있다.

* LRU (Least Recently Used) 알고리즘: 가장 오랫동안 사용되지 않은 페이지를 교체하는 방식으로, 가장 많이 사용되는 알고리즘이지만 추가적인 자료구조와 오버헤드가 필요하다.

* LFU (Least Frequently Used) 알고리즘: 가장 적게 사용된 페이지를 교체한다. 하지만 처음에 사용되지 않았던 페이지가 나중에 자주 사용되면 손해가 발생할 수 있다.

* NUR (Not Used Recently) 알고리즘: LRU를 근사하기 위한 알고리즘으로, 적은 오버헤드로 적절한 성능을 제공한다. 각 페이지마다 참조 비트와 더티 비트를 사용하여 교체한다.

**- 페이지 교체의 작동 방식**
* 물리 메모리에 모든 페이지를 올리지 못하므로 페이지는 스왑 공간이라는 디스크 공간에 저장된다. 페이지 폴트가 발생하면 해당 페이지를 디스크에서 메모리로 가져온다. 페이지 교체 정책은 페이지 폴트 발생 시 어떤 페이지를 내릴지를 결정한다.

* 운영체제는 일반적으로 여유 공간의 최댓값과 최솟값을 설정하여 페이지 교체 알고리즘을 작동시킨다. 여유 공간의 크기가 최솟값보다 작아지면 백그라운드 쓰레드인 스왑 데몬이 동작하여 페이지를 교체하고 여유 공간을 확보한다. 스왑 데몬은 여유 공간의 크기가 최댓값에 이를 때까지 페이지를 교체하며, 이를 위해 백그라운드에서 배치 작업을 수행한다.


